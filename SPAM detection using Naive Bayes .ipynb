{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh62_W0okP9w"
   },
   "source": [
    "**<center><h3>Naive Bayes Email SPAM detection Problem</h3></center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtaKdypWkYMQ"
   },
   "source": [
    "---\n",
    "# **Table of Contents**\n",
    "---\n",
    "\n",
    "**1.** [**Problem Statement**](#Section1)<br>\n",
    "**2.** [**Objective**](#Section2)<br>\n",
    "**3.** [**Installing & Importing Libraries**](#Section3)<br>\n",
    "  - **3.1** [**Installing Libraries**](#Section31)\n",
    "  - **3.2** [**Upgrading Libraries**](#Section32)\n",
    "  - **3.3** [**Importing Libraries**](#Section33)\n",
    "\n",
    "**4.** [**Data Acquisition & Description**](#Section4)<br>\n",
    "  - **4.1** [**Data Description**](#Section41)\n",
    "  - **4.2** [**Data Information**](#Section42)\n",
    "\n",
    "**5.** [**Data Pre-processing**](#Section5)<br>\n",
    "  - **5.1** [**Pre-Profiling Report**](#Section51)<br>\n",
    "\n",
    "**6.** [**Exploratory Data Analysis**](#Section6)<br>\n",
    "**7.** [**Post Data Processing & Feature Selection**](#Section7)<br>\n",
    "  - **7.1** [**Bag of Words**](#Section71)<br>\n",
    "  - **7.2** [**Count Vectorizer**](#Section72)<br>\n",
    "  - **7.3** [**Data Preparation**](#Section73)<br>\n",
    "\n",
    "**8.** [**Model Development & Evaluation**](#Section8)<br>\n",
    "**9.** [**Conclusion**](#Section9)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9k4rz6p7keYY"
   },
   "source": [
    "---\n",
    "<a name = Section1></a>\n",
    "# **1. Problem Statement**\n",
    "---\n",
    "\n",
    "- The **SMS Spam** Collection v.1 is a public set of SMS **labeled messages** that have been collected for **mobile phone spam** research.\n",
    "\n",
    "- It has one collection composed by **5,574** English, real and non-enconded **messages**, tagged according being legitimate (ham) or spam.\n",
    "\n",
    "- Identification of Ham/Spam is an **interesting usecase** of **NLP**(Natural Language Processing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2bUZrCqkhfh"
   },
   "source": [
    "---\n",
    "<a name = Section2></a>\n",
    "# **2. Objective**\n",
    "---\n",
    "\n",
    "- The objective of this assignment is to **predict** the likeliness of **email** being **spam or ham**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1Dc1X67kkGo"
   },
   "source": [
    "---\n",
    "<a name = Section3></a>\n",
    "# **3. Installing & Importing Libraries**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chnc8GQHkmf6"
   },
   "source": [
    "<a name = Section31></a>\n",
    "### **3.1 Installing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUq6i8xTbIm7"
   },
   "outputs": [],
   "source": [
    "!pip install -q datascience                   # Package that is required by pandas profiling\n",
    "!pip install -q pandas-profiling              # Library to generate basic statistics about data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeK-S4CSko6y"
   },
   "source": [
    "<a name = Section32></a>\n",
    "### **3.2 Upgrading Libraries**\n",
    "\n",
    "- **After upgrading** the libraries, you need to **restart the runtime** to make the libraries in sync. \n",
    "\n",
    "- Make sure not to execute the cell above (3.1) and below (3.2) again after restarting the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYsXt6jfkrtJ"
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pandas-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1hdcYUBktjK"
   },
   "source": [
    "<a name = Section33></a>\n",
    "### **3.3 Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFiF1yPqkvZ7"
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd                                                 # Importing for panel data analysis\n",
    "from pandas_profiling import ProfileReport                          # Import Pandas Profiling (To generate Univariate Analysis) \n",
    "pd.set_option('display.max_columns', None)                          # Unfolding hidden features if the cardinality is high      \n",
    "pd.set_option('display.max_colwidth', None)                         # Unfolding the max feature width for better clearity      \n",
    "pd.set_option('display.max_rows', None)                             # Unfolding hidden data points if the cardinality is high\n",
    "pd.set_option('mode.chained_assignment', None)                      # Removing restriction over chained assignments operations\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import numpy as np                                                  # Importing package numpys (For Numerical Python)\n",
    "import string                                                       # For string related operations\n",
    "import pprint                                                       # For printing of Collections line by line\n",
    "from collections import Counter                                     # For estimating frequency\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt                                     # Importing pyplot interface using matplotlib                                              \n",
    "import seaborn as sns                                               # Importin seaborm library for interactive visualization\n",
    "%matplotlib inline\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer         # To perform bag of words from data\n",
    "from sklearn.metrics import precision_recall_curve                  # For precision and recall metric estimation\n",
    "from sklearn.metrics import classification_report                   # To generate complete report of evaluation metrics\n",
    "from sklearn.metrics import plot_confusion_matrix                   # To plot confusion matrix \n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split                # To split the data in training and testing part     \n",
    "from sklearn.naive_bayes import MultinomialNB                       # To create a naive bayes model using algorithm\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "import warnings                                                     # Importing warning to disable runtime warnings\n",
    "warnings.filterwarnings(\"ignore\")                                   # Warnings will appear only once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOZDGO_bk7ty"
   },
   "source": [
    "---\n",
    "<a name = Section4></a>\n",
    "# **4. Data Acquisition & Description**\n",
    "---\n",
    "\n",
    "- This corpus has been collected from free for research sources at the Internet and it can be retrieved from the attached <a href = \"https://raw.githubusercontent.com/insaid2018/Term-3/master/Data/Assignment/spam.csv\">**link**</a>.\n",
    "\n",
    "| Records | Features | Dataset Size |\n",
    "| :-- | :-- | :-- |\n",
    "| 5572 | 5 | 491 KB| \n",
    "\n",
    "|Id|Feature|Description|\n",
    "|:--|:--|:--|\n",
    "|01|**v1**|Whether an email is ham or spam. Contains: [ham, spam]|\n",
    "|02|**v2**|It is the email message sent to and fro between sender and receiver.|\n",
    "|03|**Unnamed: 2**|Irrelevant and Un-necessary feature.|\n",
    "|04|**Unnamed: 3**|Irrelevant and Un-necessary feature.|\n",
    "|05|**Unnamed: 4**|Irrelevant and Un-necessary feature.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkahkxOVwXnX"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/KommuriRaju/NaiveBayes-Algo---Email-SPAM-detection/main/SPAM%20detection/Email%20SPAM%20detection', encoding = 'latin1')\n",
    "print('Data Shape:', data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5AyNVVsyFHg"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "- Before diving further let's **rename** the **features** and **drop un-necessary features**.\n",
    "\n",
    "- Additionally, we will create a **new feature** out of the Message **length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'v1':'email'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'v2':'message'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4', 'Unnamed: 0' ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Length'] = data['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8_sCdncEgw4"
   },
   "source": [
    "<a name = Section41></a>\n",
    "### **4.1 Data Description**\n",
    "\n",
    "- In this section we will get **description** and **statistics** about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dz4XNvCP2A8h"
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Et2oxpEW6xG"
   },
   "source": [
    "<a name = Section42></a>\n",
    "### **4.2 Data Information**\n",
    "\n",
    "- In this section we will get **information about the data** and see some observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNKHxPIkFT_I"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhALq5osFeDX"
   },
   "source": [
    "<a name = Section5></a>\n",
    "\n",
    "---\n",
    "# **5. Data Pre-Processing**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN6i2Yb6GYe_"
   },
   "source": [
    "<a name = Section51></a>\n",
    "### **5.1 Pre Profiling Report**\n",
    "\n",
    "- For **quick analysis** pandas profiling is very handy.\n",
    "\n",
    "- Generates profile reports from a pandas DataFrame.\n",
    "\n",
    "- For each column **statistics** are presented in an interactive HTML report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gdk4lCT2Fa4T"
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(df = data)\n",
    "#profile.to_file(output_file = 'Pre Profiling Report.html')\n",
    "print('Accomplished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djt4cuEURFr6"
   },
   "source": [
    "**Performing Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaASnXjbRLWz"
   },
   "source": [
    "**After Handling Duplicate Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dorXnRd5RK7C"
   },
   "outputs": [],
   "source": [
    "print('Contains Redundant Records?:', data.duplicated().any())\n",
    "print('Duplicate Count:', data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before dropping the duplicates, the data shape is', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After dropping the duplicates, the data shape is', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHzIs7h7SWhB"
   },
   "source": [
    "<a name = Section6></a>\n",
    "\n",
    "---\n",
    "# **6. Exploratory Data Analysis**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtSLhSHkUfBN"
   },
   "source": [
    "---\n",
    "**<h4>** Plotting the distribution of message length i.e. \"Length\".</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=[15,7])\n",
    "sns.distplot(a= data['Length'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOfw9p1SVaHZ"
   },
   "source": [
    "<a name = Section7></a>\n",
    "\n",
    "---\n",
    "# **7. Post Data Processing**\n",
    "---\n",
    "\n",
    "- Now we will **transform** our **data** into **compatible format** so that machine can understand.\n",
    "\n",
    "- Our **data** set contains **5572** number of **rows** and these will be converted into **bag of words**.\n",
    "\n",
    "- But before that we need to **clean** the **text message** by \n",
    "  \n",
    "  - Removing stopwords, punctuations, delimeters, alphanumerics,\n",
    "  - Converting all the words in lowercase.\n",
    "\n",
    "- We will then create a **vector space** of words as features and their row cells will contains the respective value (1 or 0).\n",
    "\n",
    "- We then use the **tokenized words** for each observation and find out the **frequency** of **each token**.\n",
    "\n",
    "- Example: Lets say we have 4 documents as follows:\n",
    "\n",
    "  ```\n",
    "  Hello, how are you!\n",
    "  ```\n",
    "  ```\n",
    "  Win money, win from home\n",
    "  ```\n",
    "  ```\n",
    "  Call me now\n",
    "  ```\n",
    "  ```\n",
    "  Hello, Call you tomorrow?\n",
    "  ```\n",
    "\n",
    "- Our objective here is to convert this set of text to a frequency distribution matrix, as follows:\n",
    "\n",
    "<center><img src=\"https://image.ibb.co/casG7U/countvectorizer.png\" width=70%, height=70%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8hdBEjZSLmD"
   },
   "source": [
    "<a name = Section71></a>\n",
    "## **7.1 Bag of Words**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO2DDflVZux3"
   },
   "source": [
    "<a name = Section711></a>\n",
    "### **7.1.1 Lowecase Transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CnABpsSyHB6"
   },
   "source": [
    "---\n",
    "**<h4>** Converting all the words in lower case for the give document.</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6ENU554yTHG"
   },
   "outputs": [],
   "source": [
    "documents = ['Hello, how are you!', \n",
    "             'Win money, win from home.', \n",
    "             'Call me now.', \n",
    "             'Hello, Call hello you tomorrow?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3HWD9hYymif"
   },
   "source": [
    "- Creating a function that return all the words in lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerCaseDoc = [d.lower() for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowerCaseDoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_aiM9nuab-y"
   },
   "source": [
    "<a name = Section712></a>\n",
    "### **7.1.2 Removing All Punctuations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AztvRP1Zy4mW"
   },
   "source": [
    "---\n",
    "**<h4>** Removing all the punctuations in the lowercase transformed document.</h4>\n",
    "\n",
    "---\n",
    "\n",
    "- Creating a function that returns the documents contents without punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonPunch = []\n",
    "for i in lowerCaseDoc:\n",
    "    nonPunch.append(i.translate(str.maketrans(\"\", \"\", string.punctuation)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonPunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removePunch = nonPunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeQeAfZgzSJb"
   },
   "source": [
    "- Call the function by passing the lowercase transformed document and print it's result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yqfcrs8nbbyT"
   },
   "source": [
    "<a name = Section713></a>\n",
    "### **7.1.3 Tokenization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkiDSNOuzebR"
   },
   "source": [
    "---\n",
    "**<h4>** Performing tokenization over the document on which you just removed all the punctuations.</h4>\n",
    "\n",
    "---\n",
    "\n",
    "- Creating a function that returns the contents of documents as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokendoc = [[w for w in d.split()] for d in removePunch]\n",
    "tokendoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TVe_elmdvHc"
   },
   "source": [
    "<a name = Section714></a>\n",
    "### **7.1.4 Count Frequencies**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkHotas10btC"
   },
   "source": [
    "---\n",
    "**<h4>** Estimating the frequencies of each token with the help of Counter method defined in Collections library.</h4>\n",
    "\n",
    "---\n",
    "\n",
    "- Creating a function that returns the count frequency of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[Counter(d) for d in tokendoc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTrzi10me2w2"
   },
   "source": [
    "<a name = Section72></a>\n",
    "## **7.2 Count Vectorizer**\n",
    "\n",
    "- In the above section you **saw** a version of the **CountVectorizer(**) method **from** the **scratch**.\n",
    "\n",
    "- But we can do **similar steps** with the help of CountVectorizer() with **one line** of **code**.\n",
    "\n",
    "- CountVectorizer() has certain parameters which take care of these steps for us which are as follows:\n",
    "\n",
    "  ```\n",
    "  lowercase: The lowercase parameter has a default value of True which converts all of our text to its lower case form.\n",
    "  ```\n",
    "  ```\n",
    "  token_pattern: The token_pattern parameter has a default regular expression value of - (?u)\\\\b\\\\w\\\\w+\\\\b which ignores \n",
    "                 all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length \n",
    "                 greater than or equal to 2, as individual tokens or words.\n",
    "  ```\n",
    "  ```\n",
    "  stop_words: The stop_words parameter, if set to english will remove all words from our document set that match a list of \n",
    "              English stop words which is defined in scikit-learn. \n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQs-Vlob07-4"
   },
   "source": [
    "---\n",
    "**<h4>** Performing the count vectorization over the documents feature created earlier.</h4>\n",
    "\n",
    "---\n",
    "\n",
    "- Creating a feature as an object for CountVectorizer() and pass the respective parameters.\n",
    "\n",
    "- **Fit** and **transform** the **data** to an array and finally **create a dataframe** that show the **frequency matrix**.\n",
    "\n",
    "- Make sure that the **frequency matrix** **show** the **labels** of the messsages as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countVectorTransform(data, stopwords=None):\n",
    "\n",
    "  countVector = CountVectorizer(lowercase=True, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', stop_words='english')\n",
    "\n",
    "  docArray = countVector.fit_transform(data).toarray()\n",
    "\n",
    "  frequency_matrix = pd.DataFrame(data=docArray, columns=countVector.get_feature_names())\n",
    "\n",
    "  return frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyqSh2IYh4bK"
   },
   "outputs": [],
   "source": [
    "frequency_matrix = countVectorTransform(data = documents)\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdbwunqWl8Pj"
   },
   "source": [
    "<a name = Section73></a>\n",
    "## **7.3 Data Preparation**\n",
    "\n",
    "- Now we will **split** our **data** in **training** and **testing** part for further development.\n",
    "\n",
    "- Before that you need to **convert** the **class labels** of Label features into **numeric**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DO305pCo3H67"
   },
   "source": [
    "---\n",
    "**<h4>** Performing the count vectorization over the message feature.</h4>\n",
    "\n",
    "---\n",
    "\n",
    "- **Perform label encoding** over Label feature present in data.\n",
    "\n",
    "- Take **count vectorizer transformation** over the message feature as input and Label feature as output.\n",
    "\n",
    "- **Split** the data into **80:20** along with the **stratify** parameter inside train_test_split.\n",
    "\n",
    "- Make sure to set the **random_state = 42**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i4i7Mal311K"
   },
   "outputs": [],
   "source": [
    "def dataPrep():\n",
    "  data.loc[:,'email'] = data['email'].map({'ham': 0, 'spam': 1})\n",
    "\n",
    "  X = countVectorTransform(data= data['message'], stopwords='english')\n",
    "  y = data['email']\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "  print('Training Data Shape:', X_train.shape, y_train.shape)\n",
    "  print('Testing Data Shape:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPrep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6b_-M1bBnSM_"
   },
   "source": [
    "<a name = Section8></a>\n",
    "\n",
    "---\n",
    "# **8. Model Development & Evaluation**\n",
    "---\n",
    "\n",
    "- In this section we will **develop Multinomial Naive Bayes** and **tune** our **model if required**.\n",
    "\n",
    "- Then we will **analyze the results** obtained and **make our observation**.\n",
    "\n",
    "- For **evaluation purpose** we will **focus** on **precision and recall value** for **positive class** i.e. spam class.\n",
    "\n",
    "- **Remember** that **we want generalize results** i.e. same results or error on testing data as that of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEBRt2hUnlI9"
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true, y_pred, train_or_test):\n",
    "  '''\n",
    "  y_true: Acutal values of the target\n",
    "  y_pred: Predicted values of the target. Either predict_proba or decision_function\n",
    "  line_show: Plot avergae values \"precision\" or \"recall\"\n",
    "  train_or_test: Train Data or Test Data\n",
    "  '''\n",
    "  precisions, recalls, thresholds = precision_recall_curve(y_true = y_true, probas_pred = y_pred)\n",
    "\n",
    "  average_precision = np.mean(precisions)\n",
    "  average_recall = np.mean(recalls)\n",
    "\n",
    "  sns.lineplot(x = recalls, y = precisions, linewidth = 2, ci = None)\n",
    "  plt.plot([0, 1], [average_precision, average_precision], 'r-')\n",
    "  plt.plot([average_recall, average_recall], [0, 1], 'g-')\n",
    "  plt.xlabel('Recall', fontsize = 14)\n",
    "  plt.xticks(ticks = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "  plt.yticks(ticks = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "  plt.ylabel('Precision', fontsize = 14)\n",
    "  plt.title(train_or_test, fontsize = 16)\n",
    "  plt.legend(labels = ['Binary PR Curve', 'AP {:.2f}'.format(average_precision), 'AR {:.2f}'.format(average_recall)])\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpjUN1pc4XLU"
   },
   "source": [
    "---\n",
    "**<h4>** Performing model building using Multinomial Naive Bayes over training data using default setting.</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelFit():\n",
    "  \n",
    "  naive = MultinomialNB()\n",
    "  naive.fit(X_train, y_train)\n",
    "\n",
    "  return naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pg0qvO-FvvBe"
   },
   "outputs": [],
   "source": [
    "model = modelFit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwwITXJo4iqT"
   },
   "source": [
    "---\n",
    "**<h4>** Generating a confusion matrix over the training data as well as on testing data.</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CfAED9WpblC"
   },
   "outputs": [],
   "source": [
    "def plotConf():\n",
    "\n",
    "  y_train_pred_count = naive.predict(X_train)\n",
    "  y_test_pred_count = naive.predict(X_test)\n",
    "\n",
    "  y_train_pred_proba = naive.predict_proba(X_train)\n",
    "  y_test_pred_proba = naive.predict_proba(X_test)\n",
    "\n",
    "  fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, sharex = False, figsize=(15, 7))\n",
    "  plot_confusion_matrix(estimator = naive, X = X_train, y_true = y_train, cmap = 'YlGnBu', ax = ax1)\n",
    "  plot_confusion_matrix(estimator = naive, X = X_test, y_true = y_test, cmap = 'YlGnBu', ax = ax2)\n",
    "  ax1.set_title(label = 'Train Data', size = 14)\n",
    "  ax2.set_title(label = 'Test Data', size = 14)\n",
    "  ax1.grid(b = False)\n",
    "  ax2.grid(b = False)\n",
    "  plt.suptitle(t = 'Confusion Matrix', size = 16)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DrqdSqUpv2Fe"
   },
   "outputs": [],
   "source": [
    "plotConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yuc12PXVnvTX"
   },
   "outputs": [],
   "source": [
    "def plotReport():\n",
    "  logistic_report_train = classification_report(y_train, y_train_pred_count)\n",
    "  logistic_report_test = classification_report(y_test, y_test_pred_count)\n",
    "  print('                    Training Report          ')\n",
    "  print(logistic_report_train)\n",
    "  print('                    Testing Report           ')\n",
    "  print(logistic_report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ob1WFVd4xpL"
   },
   "source": [
    "---\n",
    "**<h4>** Generating a classification report for training data as well as on testing data.</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "luATtr1XwUi_"
   },
   "outputs": [],
   "source": [
    "plotReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56qXGjUm4-mL"
   },
   "source": [
    "---\n",
    "**<h4>** Generating a precision and recall curve for training data as well as on testing data.</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tnWf17gMnz-c"
   },
   "outputs": [],
   "source": [
    "def plotPRCurve():\n",
    "  figure = plt.figure(figsize = [15, 7])\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plot_precision_recall(y_true = y_train, y_pred = y_train_pred_proba[:,1], train_or_test ='Train Data')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plot_precision_recall(y_true = y_test, y_pred = y_test_pred_proba[:, 1], train_or_test = 'Test Data')\n",
    "\n",
    "  plt.suptitle('Precision & Recall Curve', size = 16)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6MOMBVmwc4O"
   },
   "outputs": [],
   "source": [
    "plotPRCurve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oqv0vpcXhJp"
   },
   "source": [
    "<a name = Section9></a>\n",
    "\n",
    "---\n",
    "# **9. Conclusion**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3p0byDJaXnB5"
   },
   "source": [
    "- We can see that **model** is **working pretty well** in **identifying** **spam** messages.\n",
    "\n",
    "- This **model** now can **help us** in **identifying** which **message** is **spam** and which one is not."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Naive Bayes Assignment Problem.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
